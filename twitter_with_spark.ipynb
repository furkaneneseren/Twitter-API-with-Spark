{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0d0fd-6726-44c8-80c9-3bdffef9d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873593d6-2a1f-4699-a40f-79fed26c3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4dc9419-5953-4508-bdb0-55c74756f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\python\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-11-14 19:46:10 =========\n",
      "+--------------------+-----+\n",
      "|                word|total|\n",
      "+--------------------+-----+\n",
      "|               #Math|    1|\n",
      "|                 not|    1|\n",
      "|                you!|    1|\n",
      "|                 #ML|    1|\n",
      "|             #Python|    1|\n",
      "|             already|    1|\n",
      "|              You're|    2|\n",
      "|             talking|    1|\n",
      "|                #IoT|    1|\n",
      "|                  is|    2|\n",
      "|https://t.co/Jmdc...|    1|\n",
      "|                   @|    1|\n",
      "|          listening.|    1|\n",
      "|                data|    1|\n",
      "|        #Engineering|    1|\n",
      "|            probably|    1|\n",
      "|https://t.co/WMtX...|    1|\n",
      "|          Consulting|    1|\n",
      "|             #Pandas|    1|\n",
      "|        #DataScience|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2022-11-14 19:46:20 =========\n",
      "+--------------------+-----+\n",
      "|                word|total|\n",
      "+--------------------+-----+\n",
      "|               #Math|    1|\n",
      "|                 not|    1|\n",
      "|                you!|    1|\n",
      "|                 #ML|    1|\n",
      "|             #Python|    1|\n",
      "|             already|    1|\n",
      "|              You're|    2|\n",
      "|             talking|    1|\n",
      "|                #IoT|    1|\n",
      "|                  is|    2|\n",
      "|https://t.co/Jmdc...|    1|\n",
      "|                   @|    1|\n",
      "|          listening.|    1|\n",
      "|                data|    1|\n",
      "|        #Engineering|    1|\n",
      "|            probably|    1|\n",
      "|https://t.co/WMtX...|    1|\n",
      "|          Consulting|    1|\n",
      "|             #Pandas|    1|\n",
      "|        #DataScience|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2022-11-14 19:46:30 =========\n",
      "+--------------------+-----+\n",
      "|                word|total|\n",
      "+--------------------+-----+\n",
      "|https://t.co/Prkg...|    1|\n",
      "|             account|    1|\n",
      "|                 you|    2|\n",
      "|                 can|    1|\n",
      "|                 was|    2|\n",
      "|              system|    1|\n",
      "|                 too|    2|\n",
      "|                 fix|    1|\n",
      "|                they|    2|\n",
      "|                 had|    2|\n",
      "|            rendered|    1|\n",
      "|                  me|    2|\n",
      "|       problems,I’ve|    1|\n",
      "|                  on|    3|\n",
      "|                 but|    1|\n",
      "|           Instagram|    2|\n",
      "|             support|    3|\n",
      "|             similar|    1|\n",
      "|                  my|    4|\n",
      "|              helped|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2022-11-14 19:46:40 =========\n",
      "+--------------------+-----+\n",
      "|                word|total|\n",
      "+--------------------+-----+\n",
      "|        #apachespark|    1|\n",
      "|                  #…|    1|\n",
      "|               Steps|    1|\n",
      "|        #programming|    1|\n",
      "|              Apache|    1|\n",
      "|                  on|    2|\n",
      "|     @bigdata_engnr:|    1|\n",
      "|        Installation|    1|\n",
      "|https://t.co/x8Uk...|    1|\n",
      "|             #hadoop|    1|\n",
      "|              Ubuntu|    1|\n",
      "|               3.3.1|    1|\n",
      "|                    |    5|\n",
      "|            #bigdata|    1|\n",
      "|              Hadoop|    1|\n",
      "|                  RT|    2|\n",
      "|                  us|    1|\n",
      "|              *Essay|    1|\n",
      "|    @Gregory_essays:|    1|\n",
      "|                  c…|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2022-11-14 19:46:50 =========\n",
      "+-------------+-----+\n",
      "|         word|total|\n",
      "+-------------+-----+\n",
      "|       online|    1|\n",
      "|         used|    1|\n",
      "|@RobbersonJon|    1|\n",
      "|       store.|    1|\n",
      "|          for|    1|\n",
      "|         with|    1|\n",
      "|        pyrex|    1|\n",
      "|         lids|    1|\n",
      "|       bowls.|    1|\n",
      "|           $5|    1|\n",
      "|        About|    1|\n",
      "|        cover|    1|\n",
      "|         Wrap|    1|\n",
      "|        Saran|    1|\n",
      "|          the|    1|\n",
      "|           my|    1|\n",
      "|         from|    1|\n",
      "|         them|    2|\n",
      "|        Pyrex|    1|\n",
      "|       Bought|    2|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2022-11-14 19:47:00 =========\n",
      "========= 2022-11-14 19:47:10 =========\n",
      "========= 2022-11-14 19:47:20 =========\n",
      "========= 2022-11-14 19:47:30 =========\n",
      "========= 2022-11-14 19:47:40 =========\n",
      "========= 2022-11-14 19:47:50 =========\n",
      "========= 2022-11-14 19:48:00 =========\n",
      "========= 2022-11-14 19:48:10 =========\n",
      "========= 2022-11-14 19:48:20 =========\n",
      "========= 2022-11-14 19:48:30 =========\n",
      "========= 2022-11-14 19:48:40 =========\n",
      "========= 2022-11-14 19:48:50 =========\n",
      "========= 2022-11-14 19:49:00 =========\n",
      "========= 2022-11-14 19:49:10 =========\n",
      "========= 2022-11-14 19:49:20 =========\n",
      "========= 2022-11-14 19:49:30 =========\n",
      "========= 2022-11-14 19:49:40 =========\n",
      "========= 2022-11-14 19:49:50 =========\n",
      "========= 2022-11-14 19:50:00 =========\n",
      "========= 2022-11-14 19:50:10 =========\n",
      "========= 2022-11-14 19:50:20 =========\n",
      "========= 2022-11-14 19:50:30 =========\n",
      "========= 2022-11-14 19:50:40 =========\n",
      "========= 2022-11-14 19:50:50 =========\n",
      "========= 2022-11-14 19:51:00 =========\n",
      "========= 2022-11-14 19:51:10 =========\n",
      "========= 2022-11-14 19:51:20 =========\n",
      "========= 2022-11-14 19:51:30 =========\n",
      "========= 2022-11-14 19:51:40 =========\n",
      "========= 2022-11-14 19:51:50 =========\n",
      "========= 2022-11-14 19:52:00 =========\n",
      "========= 2022-11-14 19:52:10 =========\n",
      "========= 2022-11-14 19:52:20 =========\n",
      "========= 2022-11-14 19:52:30 =========\n",
      "========= 2022-11-14 19:52:40 =========\n",
      "========= 2022-11-14 19:52:50 =========\n",
      "========= 2022-11-14 19:53:00 =========\n",
      "========= 2022-11-14 19:53:10 =========\n",
      "========= 2022-11-14 19:53:20 =========\n",
      "========= 2022-11-14 19:53:30 =========\n",
      "========= 2022-11-14 19:53:40 =========\n",
      "========= 2022-11-14 19:53:50 =========\n",
      "========= 2022-11-14 19:54:00 =========\n",
      "========= 2022-11-14 19:54:10 =========\n",
      "========= 2022-11-14 19:54:20 =========\n",
      "========= 2022-11-14 19:54:30 =========\n",
      "========= 2022-11-14 19:54:40 =========\n",
      "========= 2022-11-14 19:54:50 =========\n",
      "========= 2022-11-14 19:55:00 =========\n",
      "========= 2022-11-14 19:55:10 =========\n",
      "========= 2022-11-14 19:55:20 =========\n",
      "========= 2022-11-14 19:55:30 =========\n",
      "========= 2022-11-14 19:55:40 =========\n",
      "========= 2022-11-14 19:55:50 =========\n",
      "========= 2022-11-14 19:56:00 =========\n",
      "========= 2022-11-14 19:56:10 =========\n",
      "========= 2022-11-14 19:56:20 =========\n",
      "========= 2022-11-14 19:56:30 =========\n",
      "========= 2022-11-14 19:56:40 =========\n",
      "========= 2022-11-14 19:56:50 =========\n",
      "========= 2022-11-14 19:57:00 =========\n",
      "========= 2022-11-14 19:57:10 =========\n",
      "========= 2022-11-14 19:57:20 =========\n",
      "========= 2022-11-14 19:57:30 =========\n",
      "========= 2022-11-14 19:57:40 =========\n",
      "========= 2022-11-14 19:57:50 =========\n",
      "========= 2022-11-14 19:58:00 =========\n",
      "========= 2022-11-14 19:58:10 =========\n",
      "========= 2022-11-14 19:58:20 =========\n",
      "========= 2022-11-14 19:58:30 =========\n",
      "========= 2022-11-14 19:58:40 =========\n",
      "========= 2022-11-14 19:58:50 =========\n",
      "========= 2022-11-14 19:59:00 =========\n",
      "========= 2022-11-14 19:59:10 =========\n",
      "========= 2022-11-14 19:59:20 =========\n",
      "========= 2022-11-14 19:59:30 =========\n",
      "========= 2022-11-14 19:59:40 =========\n",
      "========= 2022-11-14 19:59:50 =========\n",
      "========= 2022-11-14 20:00:00 =========\n",
      "========= 2022-11-14 20:00:10 =========\n",
      "========= 2022-11-14 20:00:20 =========\n",
      "========= 2022-11-14 20:00:30 =========\n",
      "========= 2022-11-14 20:00:40 =========\n",
      "========= 2022-11-14 20:00:50 =========\n",
      "========= 2022-11-14 20:01:00 =========\n",
      "========= 2022-11-14 20:01:10 =========\n",
      "========= 2022-11-14 20:01:20 =========\n",
      "========= 2022-11-14 20:01:30 =========\n",
      "========= 2022-11-14 20:01:40 =========\n",
      "========= 2022-11-14 20:01:50 =========\n",
      "========= 2022-11-14 20:02:00 =========\n",
      "========= 2022-11-14 20:02:10 =========\n",
      "========= 2022-11-14 20:02:20 =========\n",
      "========= 2022-11-14 20:02:30 =========\n",
      "========= 2022-11-14 20:02:40 =========\n",
      "========= 2022-11-14 20:02:50 =========\n",
      "========= 2022-11-14 20:03:00 =========\n",
      "========= 2022-11-14 20:03:10 =========\n",
      "========= 2022-11-14 20:03:20 =========\n",
      "========= 2022-11-14 20:03:30 =========\n",
      "========= 2022-11-14 20:03:40 =========\n",
      "========= 2022-11-14 20:03:50 =========\n",
      "========= 2022-11-14 20:04:00 =========\n",
      "========= 2022-11-14 20:04:10 =========\n",
      "========= 2022-11-14 20:04:20 =========\n",
      "========= 2022-11-14 20:04:30 =========\n",
      "========= 2022-11-14 20:04:40 =========\n",
      "========= 2022-11-14 20:04:50 =========\n",
      "========= 2022-11-14 20:05:00 =========\n",
      "========= 2022-11-14 20:05:10 =========\n",
      "========= 2022-11-14 20:05:20 =========\n",
      "========= 2022-11-14 20:05:30 =========\n",
      "========= 2022-11-14 20:05:40 =========\n",
      "========= 2022-11-14 20:05:50 =========\n",
      "========= 2022-11-14 20:06:00 =========\n",
      "========= 2022-11-14 20:06:10 =========\n",
      "========= 2022-11-14 20:06:20 =========\n",
      "========= 2022-11-14 20:06:30 =========\n",
      "========= 2022-11-14 20:06:40 =========\n",
      "========= 2022-11-14 20:06:50 =========\n",
      "========= 2022-11-14 20:07:00 =========\n",
      "========= 2022-11-14 20:07:10 =========\n",
      "========= 2022-11-14 20:07:20 =========\n",
      "========= 2022-11-14 20:07:30 =========\n",
      "========= 2022-11-14 20:07:40 =========\n",
      "========= 2022-11-14 20:07:50 =========\n",
      "========= 2022-11-14 20:08:00 =========\n",
      "========= 2022-11-14 20:08:10 =========\n",
      "========= 2022-11-14 20:08:20 =========\n",
      "========= 2022-11-14 20:08:30 =========\n",
      "========= 2022-11-14 20:08:40 =========\n",
      "========= 2022-11-14 20:08:50 =========\n",
      "========= 2022-11-14 20:09:00 =========\n",
      "========= 2022-11-14 20:09:10 =========\n",
      "========= 2022-11-14 20:09:20 =========\n",
      "========= 2022-11-14 20:09:30 =========\n",
      "========= 2022-11-14 20:09:40 =========\n",
      "========= 2022-11-14 20:09:50 =========\n",
      "========= 2022-11-14 20:10:00 =========\n",
      "========= 2022-11-14 20:10:10 =========\n",
      "========= 2022-11-14 20:10:20 =========\n",
      "========= 2022-11-14 20:10:30 =========\n",
      "========= 2022-11-14 20:10:40 =========\n",
      "========= 2022-11-14 20:10:50 =========\n",
      "========= 2022-11-14 20:11:00 =========\n",
      "========= 2022-11-14 20:11:10 =========\n",
      "========= 2022-11-14 20:11:20 =========\n",
      "========= 2022-11-14 20:11:30 =========\n",
      "========= 2022-11-14 20:11:40 =========\n",
      "========= 2022-11-14 20:11:50 =========\n",
      "========= 2022-11-14 20:12:00 =========\n",
      "========= 2022-11-14 20:12:10 =========\n",
      "========= 2022-11-14 20:12:20 =========\n",
      "========= 2022-11-14 20:12:30 =========\n",
      "========= 2022-11-14 20:12:40 =========\n",
      "========= 2022-11-14 20:12:50 =========\n",
      "========= 2022-11-14 20:13:00 =========\n",
      "========= 2022-11-14 20:13:10 =========\n",
      "========= 2022-11-14 20:13:20 =========\n",
      "========= 2022-11-14 20:13:30 =========\n",
      "========= 2022-11-14 20:13:40 =========\n",
      "========= 2022-11-14 20:13:50 =========\n",
      "========= 2022-11-14 20:14:00 =========\n",
      "========= 2022-11-14 20:14:10 =========\n",
      "========= 2022-11-14 20:14:20 =========\n",
      "========= 2022-11-14 20:14:30 =========\n",
      "========= 2022-11-14 20:14:40 =========\n",
      "========= 2022-11-14 20:14:50 =========\n",
      "========= 2022-11-14 20:15:00 =========\n",
      "========= 2022-11-14 20:15:10 =========\n",
      "========= 2022-11-14 20:15:20 =========\n",
      "========= 2022-11-14 20:15:30 =========\n",
      "========= 2022-11-14 20:15:40 =========\n",
      "========= 2022-11-14 20:15:50 =========\n",
      "========= 2022-11-14 20:16:00 =========\n",
      "========= 2022-11-14 20:16:10 =========\n",
      "========= 2022-11-14 20:16:20 =========\n",
      "========= 2022-11-14 20:16:30 =========\n",
      "========= 2022-11-14 20:16:40 =========\n",
      "========= 2022-11-14 20:16:50 =========\n",
      "========= 2022-11-14 20:17:00 =========\n",
      "========= 2022-11-14 20:17:10 =========\n",
      "========= 2022-11-14 20:17:20 =========\n",
      "========= 2022-11-14 20:17:30 =========\n",
      "========= 2022-11-14 20:17:40 =========\n",
      "========= 2022-11-14 20:17:50 =========\n",
      "========= 2022-11-14 20:18:00 =========\n",
      "========= 2022-11-14 20:18:10 =========\n",
      "========= 2022-11-14 20:18:20 =========\n",
      "========= 2022-11-14 20:18:30 =========\n",
      "========= 2022-11-14 20:18:40 =========\n",
      "========= 2022-11-14 20:18:50 =========\n",
      "========= 2022-11-14 20:19:00 =========\n",
      "========= 2022-11-14 20:19:10 =========\n",
      "========= 2022-11-14 20:19:20 =========\n",
      "========= 2022-11-14 20:19:30 =========\n",
      "========= 2022-11-14 20:19:40 =========\n",
      "========= 2022-11-14 20:19:50 =========\n",
      "========= 2022-11-14 20:20:00 =========\n",
      "========= 2022-11-14 20:20:10 =========\n",
      "========= 2022-11-14 20:20:20 =========\n",
      "========= 2022-11-14 20:20:30 =========\n",
      "========= 2022-11-14 20:20:40 =========\n",
      "========= 2022-11-14 20:20:50 =========\n",
      "========= 2022-11-14 20:21:00 =========\n",
      "========= 2022-11-14 20:21:10 =========\n",
      "========= 2022-11-14 20:21:20 =========\n",
      "========= 2022-11-14 20:21:30 =========\n",
      "========= 2022-11-14 20:21:40 =========\n",
      "========= 2022-11-14 20:21:50 =========\n",
      "========= 2022-11-14 20:22:00 =========\n",
      "========= 2022-11-14 20:22:10 =========\n",
      "========= 2022-11-14 20:22:20 =========\n",
      "========= 2022-11-14 20:22:30 =========\n",
      "========= 2022-11-14 20:22:40 =========\n",
      "========= 2022-11-14 20:22:50 =========\n",
      "========= 2022-11-14 20:23:00 =========\n",
      "========= 2022-11-14 20:23:10 =========\n",
      "========= 2022-11-14 20:23:20 =========\n",
      "========= 2022-11-14 20:23:30 =========\n",
      "========= 2022-11-14 20:23:40 =========\n",
      "========= 2022-11-14 20:23:50 =========\n",
      "========= 2022-11-14 20:24:00 =========\n",
      "========= 2022-11-14 20:24:10 =========\n",
      "========= 2022-11-14 20:24:20 =========\n",
      "========= 2022-11-14 20:24:30 =========\n",
      "========= 2022-11-14 20:24:40 =========\n",
      "========= 2022-11-14 20:24:50 =========\n",
      "========= 2022-11-14 20:25:00 =========\n",
      "========= 2022-11-14 20:25:10 =========\n",
      "========= 2022-11-14 20:25:20 =========\n",
      "========= 2022-11-14 20:25:30 =========\n",
      "========= 2022-11-14 20:25:40 =========\n",
      "========= 2022-11-14 20:25:50 =========\n",
      "========= 2022-11-14 20:26:00 =========\n",
      "========= 2022-11-14 20:26:10 =========\n",
      "========= 2022-11-14 20:26:20 =========\n",
      "========= 2022-11-14 20:26:30 =========\n",
      "========= 2022-11-14 20:26:40 =========\n",
      "========= 2022-11-14 20:26:50 =========\n",
      "========= 2022-11-14 20:27:00 =========\n",
      "========= 2022-11-14 20:27:10 =========\n",
      "========= 2022-11-14 20:27:20 =========\n",
      "========= 2022-11-14 20:27:30 =========\n",
      "========= 2022-11-14 20:27:40 =========\n",
      "========= 2022-11-14 20:27:50 =========\n",
      "========= 2022-11-14 20:28:00 =========\n",
      "========= 2022-11-14 20:28:10 =========\n",
      "========= 2022-11-14 20:28:20 =========\n",
      "========= 2022-11-14 20:28:30 =========\n",
      "========= 2022-11-14 20:28:40 =========\n",
      "========= 2022-11-14 20:28:50 =========\n",
      "========= 2022-11-14 20:29:00 =========\n",
      "========= 2022-11-14 20:29:10 =========\n",
      "========= 2022-11-14 20:29:20 =========\n",
      "========= 2022-11-14 20:29:30 =========\n",
      "========= 2022-11-14 20:29:40 =========\n",
      "========= 2022-11-14 20:29:50 =========\n",
      "========= 2022-11-14 20:30:00 =========\n",
      "========= 2022-11-14 20:30:10 =========\n",
      "========= 2022-11-14 20:30:20 =========\n",
      "========= 2022-11-14 20:30:30 =========\n",
      "========= 2022-11-14 20:30:40 =========\n",
      "========= 2022-11-14 20:30:50 =========\n",
      "========= 2022-11-14 20:31:00 =========\n",
      "========= 2022-11-14 20:31:10 =========\n",
      "========= 2022-11-14 20:31:20 =========\n",
      "========= 2022-11-14 20:31:30 =========\n",
      "========= 2022-11-14 20:31:40 =========\n"
     ]
    }
   ],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.rdd import RDD\n",
    "import datetime\n",
    "\n",
    "def getSparkSessionInstance(sparkConf: SparkConf) -> SparkSession:\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "\n",
    "# Can only run this once. restart your kernel for any errors.\n",
    "sc = SparkContext(appName = \"Tweeter\")\n",
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5554)\n",
    "lines = socket_stream.window( 20 )\n",
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Use Parenthesis for multiple lines or use \\.\n",
    "\"\"\"( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  .filter( lambda word: word.lower().startswith(\"#\") ) # Checks for hashtag calls\n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) # Reduces\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) ) # Stores in a Tweet Object\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( (\"count\") ) # Sorts Them in a DF\n",
    "  .limit(10).createOrReplaceTempView (\"tweets\") ) ) # Registers to a table\n",
    "\"\"\"\n",
    "def process(time: datetime.datetime, rdd) -> None:\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "\n",
    "    try:\n",
    "        # Get the singleton instance of SparkSession\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda w: Row(word=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame.\n",
    "        wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = \\\n",
    "            spark.sql(\"select word, count(*) as total from words group by word\")\n",
    "        wordCountsDataFrame.show()\n",
    "    except BaseException:\n",
    "        pass\n",
    "\n",
    "words.foreachRDD(process)\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f3cbac",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\r\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:226)\r\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:67)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.<init>(ForEachDStream.scala:39)\r\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:655)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$3(DStream.scala:640)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\r\n\tat org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:264)\r\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:640)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.callForeachRDD(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD(PythonDStream.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lines\u001b[39m.\u001b[39;49mpprint()\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\streaming\\dstream.py:180\u001b[0m, in \u001b[0;36mDStream.pprint\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    178\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 180\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforeachRDD(takeAndPrint)\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\streaming\\dstream.py:158\u001b[0m, in \u001b[0;36mDStream.foreachRDD\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    156\u001b[0m jfunc \u001b[39m=\u001b[39m TransformFunction(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc, func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer)\n\u001b[0;32m    157\u001b[0m api \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ssc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonDStream\n\u001b[1;32m--> 158\u001b[0m api\u001b[39m.\u001b[39;49mcallForeachRDD(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdstream, jfunc)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\r\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:226)\r\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:67)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.<init>(ForEachDStream.scala:39)\r\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:655)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$3(DStream.scala:640)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\r\n\tat org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:264)\r\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:640)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.callForeachRDD(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD(PythonDStream.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "lines.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49bbe109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509a35a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: tweets; line 1 pos 23;\n'Project ['tag, 'count]\n+- 'UnresolvedRelation [tweets], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[0;32m      4\u001b[0m     time\u001b[39m.\u001b[39msleep( \u001b[39m3\u001b[39m )\n\u001b[1;32m----> 5\u001b[0m     top_10_tweets \u001b[39m=\u001b[39m sqlContext\u001b[39m.\u001b[39;49msql( \u001b[39m'\u001b[39;49m\u001b[39mSelect tag, count from tweets\u001b[39;49m\u001b[39m'\u001b[39;49m )\n\u001b[0;32m      6\u001b[0m     top_10_df \u001b[39m=\u001b[39m top_10_tweets\u001b[39m.\u001b[39mtoPandas()\n\u001b[0;32m      7\u001b[0m     display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\context.py:437\u001b[0m, in \u001b[0;36mSQLContext.sql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msql\u001b[39m(\u001b[39mself\u001b[39m, sqlQuery):\n\u001b[0;32m    422\u001b[0m     \u001b[39m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \n\u001b[0;32m    424\u001b[0m \u001b[39m    .. versionadded:: 1.0.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparkSession\u001b[39m.\u001b[39;49msql(sqlQuery)\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msql\u001b[39m(\u001b[39mself\u001b[39m, sqlQuery):\n\u001b[0;32m    708\u001b[0m     \u001b[39m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \n\u001b[0;32m    710\u001b[0m \u001b[39m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[39m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapped)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Table or view not found: tweets; line 1 pos 23;\n'Project ['tag, 'count]\n+- 'UnresolvedRelation [tweets], [], false\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    \n",
    "    time.sleep( 3 )\n",
    "    top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure( figsize = ( 10, 8 ) )\n",
    "#     sns.barplot(x='count',y='land_cover_specific', data=df, palette='Spectral')\n",
    "    sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    plt.show()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dbce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b04f0dddd9f19c3080a8e843bfac23ef96c1453e4f59dfcd7e6dd8f82dee673"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
