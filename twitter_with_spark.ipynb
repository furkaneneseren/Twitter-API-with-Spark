{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0d0fd-6726-44c8-80c9-3bdffef9d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873593d6-2a1f-4699-a40f-79fed26c3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4dc9419-5953-4508-bdb0-55c74756f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\python\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-01-03 22:49:00 =========\n",
      "========= 2023-01-03 22:49:10 =========\n",
      "========= 2023-01-03 22:49:20 =========\n",
      "========= 2023-01-03 22:49:30 =========\n",
      "========= 2023-01-03 22:49:40 =========\n",
      "========= 2023-01-03 22:49:50 =========\n"
     ]
    }
   ],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.rdd import RDD\n",
    "import datetime\n",
    "\n",
    "def getSparkSessionInstance(sparkConf: SparkConf) -> SparkSession:\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "\n",
    "# Can only run this once. restart your kernel for any errors.\n",
    "sc = SparkContext(appName = \"Tweeter\")\n",
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5554)\n",
    "lines = socket_stream.window( 20 )\n",
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).filter( lambda word: word.lower().startswith(\"#\") )\n",
    "\n",
    "# Use Parenthesis for multiple lines or use \\.\n",
    "\"\"\"( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  .filter( lambda word: word.lower().startswith(\"#\") ) # Checks for hashtag calls\n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) # Reduces\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) ) # Stores in a Tweet Object\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( (\"count\") ) # Sorts Them in a DF\n",
    "  .limit(10).createOrReplaceTempView (\"tweets\") ) ) # Registers to a table\n",
    "\"\"\"\n",
    "def process(time: datetime.datetime, rdd) -> None:\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "\n",
    "    try:\n",
    "        # Get the singleton instance of SparkSession\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda w: Row(word=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame.\n",
    "        wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = \\\n",
    "            spark.sql(\"select word, count(*) as total from words group by word\")\n",
    "        top_10_df = wordCountsDataFrame.toPandas()\n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure( figsize = ( 10, 8 ) )\n",
    "    #     sns.barplot(x='count',y='land_cover_specific', data=df, palette='Spectral')\n",
    "        sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "        plt.show()\n",
    "    except BaseException:\n",
    "        pass\n",
    "\n",
    "words.foreachRDD(process)\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32195415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-01-03 22:29:20 =========\n",
      "+-------------+-----+\n",
      "|         word|total|\n",
      "+-------------+-----+\n",
      "|  #BackToWork|    1|\n",
      "|#100DaysOfEdD|    1|\n",
      "|  #holistic3d|    1|\n",
      "+-------------+-----+\n",
      "\n",
      "========= 2023-01-03 22:29:30 =========\n",
      "+-------------+-----+\n",
      "|         word|total|\n",
      "+-------------+-----+\n",
      "|  #BackToWork|    1|\n",
      "|#100DaysOfEdD|    1|\n",
      "|  #holistic3d|    1|\n",
      "|        #team|    1|\n",
      "|       #inno…|    1|\n",
      "|        #PaaS|    1|\n",
      "+-------------+-----+\n",
      "\n",
      "========= 2023-01-03 22:29:40 =========\n",
      "+------+-----+\n",
      "|  word|total|\n",
      "+------+-----+\n",
      "| #team|    2|\n",
      "|#inno…|    2|\n",
      "| #PaaS|    2|\n",
      "+------+-----+\n",
      "\n",
      "========= 2023-01-03 22:29:50 =========\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3cbac",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\r\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:226)\r\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:67)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.<init>(ForEachDStream.scala:39)\r\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:655)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$3(DStream.scala:640)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\r\n\tat org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:264)\r\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:640)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.callForeachRDD(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD(PythonDStream.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lines\u001b[39m.\u001b[39;49mpprint()\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\streaming\\dstream.py:180\u001b[0m, in \u001b[0;36mDStream.pprint\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    178\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 180\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforeachRDD(takeAndPrint)\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\streaming\\dstream.py:158\u001b[0m, in \u001b[0;36mDStream.foreachRDD\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    156\u001b[0m jfunc \u001b[39m=\u001b[39m TransformFunction(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc, func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer)\n\u001b[0;32m    157\u001b[0m api \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ssc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonDStream\n\u001b[1;32m--> 158\u001b[0m api\u001b[39m.\u001b[39;49mcallForeachRDD(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdstream, jfunc)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\r\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:226)\r\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:67)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.<init>(ForEachDStream.scala:39)\r\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:655)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$3(DStream.scala:640)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\r\n\tat org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:264)\r\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:640)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.callForeachRDD(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD(PythonDStream.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "lines.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bbe109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509a35a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[0;32m      5\u001b[0m     time\u001b[39m.\u001b[39msleep( \u001b[39m3\u001b[39m )\n\u001b[1;32m----> 6\u001b[0m     top_10_tweets \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mselect word, count(*) as total from words group by word order by total\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     top_10_df \u001b[39m=\u001b[39m top_10_tweets\u001b[39m.\u001b[39mtoPandas()\n\u001b[0;32m      8\u001b[0m     display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-01-03 22:38:50 =========\n",
      "+--------+-----+\n",
      "|    word|total|\n",
      "+--------+-----+\n",
      "|#Polygon|    1|\n",
      "|#Binance|    1|\n",
      "|  #MATIC|    1|\n",
      "+--------+-----+\n",
      "\n",
      "========= 2023-01-03 22:39:00 =========\n",
      "+--------+-----+\n",
      "|    word|total|\n",
      "+--------+-----+\n",
      "|#Polygon|    1|\n",
      "|#Binance|    1|\n",
      "|  #MATIC|    1|\n",
      "+--------+-----+\n",
      "\n",
      "========= 2023-01-03 22:39:10 =========\n",
      "========= 2023-01-03 22:39:20 =========\n",
      "========= 2023-01-03 22:39:30 =========\n",
      "========= 2023-01-03 22:39:40 =========\n",
      "========= 2023-01-03 22:39:50 =========\n",
      "========= 2023-01-03 22:40:00 =========\n",
      "========= 2023-01-03 22:40:10 =========\n",
      "========= 2023-01-03 22:40:20 =========\n",
      "========= 2023-01-03 22:40:30 =========\n",
      "========= 2023-01-03 22:40:40 =========\n",
      "========= 2023-01-03 22:40:50 =========\n",
      "========= 2023-01-03 22:41:00 =========\n",
      "========= 2023-01-03 22:41:10 =========\n",
      "========= 2023-01-03 22:41:20 =========\n",
      "========= 2023-01-03 22:41:30 =========\n",
      "========= 2023-01-03 22:41:40 =========\n",
      "========= 2023-01-03 22:41:50 =========\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    \n",
    "    time.sleep( 3 )\n",
    "    top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure( figsize = ( 10, 8 ) )\n",
    "#     sns.barplot(x='count',y='land_cover_specific', data=df, palette='Spectral')\n",
    "    sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    plt.show()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dbce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2af8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (tags/v3.8.3:6f8c832, May 13 2020, 22:20:19) [MSC v.1925 32 bit (Intel)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b04f0dddd9f19c3080a8e843bfac23ef96c1453e4f59dfcd7e6dd8f82dee673"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
